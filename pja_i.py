# -*- coding: utf-8 -*-
"""PJA I

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yNTC4Tk9SBSELh46qiEFB_oFJC8rd4y6
"""

# OBJETIVO GERAL: Script em Python para uma analise do projeto aplicado, unindo dados de imóveis (QuintoAndar) com base
# em bairros/distritos e cruzando com ocorrências criminais. O script carrega dados, limpa, padroniza
# nomes, cria versões longas, gera rankings, faz merges temporais e produz gráficos e estatísticas.

"""OBJETIVO GERAL: Analisar a correlação entre os preços dos imóveis e os índices de criminalidade na cidade de São Paulo, utilizando dados de imóveis da imobiliária Quinto Andar e dados de criminalidade fornecidos pela Secretaria de Segurança Publica do Estado de São Paulo, aplicando técnicas de análise de dados para identificar padrões e possíveis influências entre essas variáveis."""

# --- IMPORTS INICIAIS ---
# OBJETIVO: Importar bibliotecas essenciais.
# Usamos pandas para manipulação tabular e os para manipulação de caminhos/arquivos.
import pandas as pd
import os

# --- FUNÇÕES AUXILIARES ---
# OBJETIVO: Funções utilitárias reutilizáveis.
# Função 'traduz' serve para transformar dtype do pandas em rótulos legíveis para o relatório de metadados.
def traduz(tipo):
  # 1) Verifica se o dtype contém 'int' -> rotula como Número inteiro.
    if "int" in str(tipo):
        return "Número inteiro"
   # 2) Verifica se o dtype contém 'float' -> rotula como Número decimal.
    elif "float" in str(tipo):
        return "Número decimal"
  # 3) Verifica se o dtype contém 'bool' -> rotula como Booleano.
    elif "bool" in str(tipo):
   # 4) Caso contrário -> Texto.
        return "Booleano"
    else:
        return "Texto"

# --- LEITURA DE DADOS ---
# OBJETIVO: Carregar os arquivos principais do Google Drive/Colab para objetos pandas.

# 1) Carrega planilha de ocorrências criminais por distrito.
ocorrencias = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/dados_ocorrencias_distritos.xlsx')

# 2) Carrega CSV com dados de imóveis (QuintoAndar exportado).
dados = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.csv')

# 3) Carrega planilha que contém distritos e bairros. sheet_name=None -> todos as abas em dict.
distritos_bairros = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/distritos_bairros.xlsx', sheet_name=None)

# 4) Carrega planilha do QuintoAndar (múltiplas abas) como dict de DataFrames.
quintoAndar = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/quintoAndar.xlsx', sheet_name=None)

# 5) Carrega relatórios trimestrais com múltiplas abas.
relatorioTri = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/relatoriosTrimestrais.xlsx', sheet_name=None)

# --- AGRUPAMENTO INICIAL DE DATASETS ---
# OBJETIVO: Construir um dicionário 'datasets' que facilità iterações (ex: extração de metadados).
datasets = {
    "ocorrencias": ocorrencias,
    "dados": dados
}

# OBJETIVO: Acrescentar cada aba do quintoAndar ao dicionário 'datasets' com prefixo identificador.
for aba, df in quintoAndar.items():
    datasets[f"quintoAndar_{aba}"] = df

# OBJETIVO: Acrescentar cada aba de relatorioTri ao dicionário 'datasets' com prefixo.
for aba, df in relatorioTri.items():
    datasets[f"relatorioTri_{aba}"] = df

# OBJETIVO: Acrescentar cada aba de distritos_bairros ao dicionário 'datasets' com prefixo.
for aba, df in distritos_bairros.items():
    datasets[f"distritos_bairros_{aba}"] = df

# --- COLETA DE METADADOS ---
# OBJETIVO: Função para mostrar e registrar metadados (linhas, colunas, tipos, nulos) de cada dataset.
relatorio_metadados = []

def mostra_metadados(df, nome):
    # OBJETIVO da função: exibir e armazenar metadados para auditoria e documentação.
    print(f"\n=== Dataset: {nome} ===")
    print("\nPrimeiras linhas:")
    print(df.head())

    print("\nMetadados:")
    print(f"Linhas: {df.shape[0]}, Colunas: {df.shape[1]}")
    print("Colunas, tipo de dado e quantidade de valores nulos:")

    for col in df.columns:
        # 1) Conta nulos por coluna.
        nulos = df[col].isnull().sum()
        # 2) Traduz dtype para rótulo legível.
        tipo = traduz(df[col].dtype)
        # 3) Adiciona ao relatório de metadados (lista de dicionários).
        relatorio_metadados.append({
            "Dataset": nome,
            "Coluna": col,
            "Tipo de Dado": tipo,
            "Valores Nulos": nulos,
            "Total de Linhas": df.shape[0],
            "Total de Colunas": df.shape[1]
        })
        print(f" - {col}: {tipo}, Nulos: {nulos}")
    print("-" * 60)

# OBJETIVO: Iterar sobre o dicionário 'datasets' e exibir metadados de cada DataFrame.
for nome, df in datasets.items():
    mostra_metadados(df, nome)

# --- RENOMEAÇÃO DE COLUNAS (PADRONIZAÇÃO) ---
# OBJETIVO: Mostrar colunas originais do DataFrame 'dados' para referência antes da renomeação.
print("Colunas originais:")
print(dados.columns.tolist())

# OBJETIVO: Dicionário de renomeação para trazer colunas para Português e nomes padronizados.
novos_nomes = {
    "address": "endereço",
    "district": "bairros",
    "area": "área m²",
    "bedrooms": "quartos",
    "garage": "garagem",
    "type": "tipo",
    "rent": "aluguel",
    "total": "total"
}

# 1) Aplica renomeação in-place para atualizar o DataFrame de imóveis.
dados.rename(columns=novos_nomes, inplace=True)

print("\nColunas renomeadas com sucesso:")
print(dados.columns.tolist())

# --- INSTALAÇÃO E IMPORTS PARA LIMPEZA DE TEXTO ---
# OBJETIVO: Assegurar que a biblioteca unidecode esteja disponível para remoção de acentos.
!pip install unidecode
import re
import unidecode

# --- LIMPEZA E PADRONIZAÇÃO DA COLUNA 'bairros' DO DATASET DE IMÓVEIS ---
# --- ETAPA DE LIMPEZA E PADRONIZAÇÃO DOS NOMES DE BAIRROS ---
# OBJETIVO: Garantir que os nomes de bairros do QuintoAndar (ex: "Jd. Paulista (Zona Oeste)")
# fiquem idênticos aos da nossa tabela de referência (ex: "jardim paulista") para que a junção (merge) funcione.

# 0) Cria cópia da coluna original para auditoria (poder reverter se necessário).
dados["bairros_original"] = dados["bairros"]

# 1) Remove informações extras (como a região entre parênteses) e espaços em branco nas extremidades.
# Por que: texto extra atrapalha comparação direta com a tabela de referência.
dados["bairros"] = dados["bairros"].astype(str).apply(lambda x: re.sub(r"\(.*?\)", "", x).strip())

# 2) Remove acentuação e caracteres especiais (ex: "Vila Carrão" -> "Vila Carrao").
# Por que: a referência pode estar sem acento; remover acentos uniformiza comparações.
dados["bairros"] = dados["bairros"].apply(lambda x: unidecode.unidecode(x))

# 3) Converte todos os nomes para letras minúsculas para padronizar (ex: "Pinheiros" -> "pinheiros").
# Por que: evita diferença entre 'Pinheiros' e 'pinheiros' no merge.
dados["bairros"] = dados["bairros"].str.lower()

# OBJETIVO: Visualizar e auditar os resultados da limpeza de bairros.
print("Bairros após limpeza:")
print(dados["bairros"].unique()[:30])

# Ver bairros únicos
print(f"Total de bairros distintos: {dados['bairros'].nunique()}")
print(dados['bairros'].sort_values().unique())

# --- PREPARAÇÃO DA TABELA DE REFERÊNCIA DE BAIRROS (distritos_bairros) ---
# OBJETIVO: Selecionar a aba correta que contém a coluna de bairros de referência.
# Observação: assumimos que existe uma aba chamada "Bairro" com a lista oficial.
bairros_ref = distritos_bairros["Bairro"]

# 1) Preserve a coluna original da referência para auditoria.
bairros_ref["Bairro_original"] = bairros_ref["Bairro"]

# 2) Limpa nomes na tabela de referência: remove parênteses e espaços extras.
bairros_ref["Bairro"] = bairros_ref["Bairro"].astype(str).apply(lambda x: re.sub(r"\(.*?\)", "", x).strip())

# 3) Remove acentuação na tabela de referência.
bairros_ref["Bairro"] = bairros_ref["Bairro"].apply(lambda x: unidecode.unidecode(x))

# 4) Converte para minúsculo.
bairros_ref["Bairro"] = bairros_ref["Bairro"].str.lower()

# OBJETIVO: Mostrar amostra da tabela de referência para confirmar limpeza.
print("Amostra da coluna Bairro após limpeza:")
print(bairros_ref.head())

# OBJETIVO: Criar lista de bairros válidos a partir da referência para filtrar o dataset principal.
bairros_validos = bairros_ref["Bairro"].unique().tolist()
print(f"Total de bairros válidos: {len(bairros_validos)}")

# --- FILTRAGEM DO DATAFRAME PRINCIPAL COM BASE NA REFERÊNCIA ---
# OBJETIVO: Manter apenas imóveis cujo bairro seja reconhecido na tabela oficial.
# Por que: evita registros com bairros inválidos/externos que atrapalhariam o merge com id_distrito.
dados = dados[dados["bairros"].isin(bairros_validos)]

print(f"Dados filtrados com sucesso! {dados.shape[0]} linhas restantes.")

# --- PREPARAÇÃO DO SUBSET DA REFERÊNCIA PARA O MERGE ---
# OBJETIVO: Selecionar apenas as colunas necessárias ('Bairro' e 'id_distrito') para o merge.
bairros_ref_subset = bairros_ref[["Bairro", "id_distrito"]]

# --- JUNÇÃO (MERGE) PARA ADICIONAR id_distrito AO DATASET DE IMÓVEIS ---
# OBJETIVO: Fazer merge à esquerda para manter todos os imóveis (aplicando id quando houver match).
dados = dados.merge(bairros_ref_subset,
                    left_on="bairros",
                    right_on="Bairro",
                    how="left")

# 1) Remover coluna duplicada 'Bairro' que veio da referência para evitar ambiguidade.
dados.drop(columns=["Bairro"], inplace=True)

# 2) Exibir amostra do dataset após merge para verificação.
print(dados.head())

# 3) Verificar quantos registros ficaram sem id_distrito (NaN) — sinal que não houve match.
nan_count = dados["id_distrito"].isnull().sum()
print(f"Linhas com id_distrito NaN: {nan_count}")

# 4) Se existirem NaN em id_distrito, remover essas linhas (opção: ou tratar manualmente).
if nan_count > 0:
    dados = dados.dropna(subset=["id_distrito"])
    print(f"Linhas com id_distrito NaN removidas. Novas linhas restantes: {dados.shape[0]}")
else:
    print("Nenhum NaN encontrado na coluna id_distrito.")

# --- SALVAR DATASET LIMPO ---
# OBJETIVO: Definir caminhos e salvar o dataset tratado. CSV comentado opcionalmente.
caminho_csv = '/content/drive/MyDrive/Colab Notebooks/dados_limpos.csv'
caminho_excel = '/content/drive/MyDrive/Colab Notebooks/dados_limpos.xlsx'

# Se quiser salvar em CSV, descomente as linhas abaixo:
# dados.to_csv(caminho_csv, index=False)
# print(f"Dataset salvo em CSV: {caminho_csv}")

dados.to_excel(caminho_excel, index=False)
print(f"Dataset salvo em Excel: {caminho_excel}")

# --- INSPEÇÃO INICIAL DAS OCORRÊNCIAS CRIMINAIS ---
# OBJETIVO: Mostrar as primeiras linhas do dataset de ocorrências para inspeção rápida.
ocorrencias.head(20)

# --- PADRONIZAÇÃO DE NOMES DE DELITOS ---
# OBJETIVO: Listar os tipos únicos de delito e padronizá-los, removendo sufixos/extras.
tipos_unicos = ocorrencias["tipo_delito"].unique()
tipos_unicos = sorted(tipos_unicos)

# 1) Imprime cada tipo único para inspeção manual (útil para identificar inconsistências).
for tipo in tipos_unicos:
    print(tipo)

# 2) Dicionário de renomeações para consolidar labels inconsistentes.
renomear_delitos = {
    "HOMICÍDIO DOLOSO (2)": "HOMICÍDIO DOLOSO",
    "HOMICÍDIO CULPOSO OUTROS": "HOMICÍDIO CULPOSO",
    "FURTO - OUTROS": "FURTO",
    "ROUBO - OUTROS": "ROUBO",
    "TOTAL DE ESTUPRO (4)": "ESTUPRO",
    "TOTAL DE ROUBO - OUTROS (1)": "ROUBO",
}

# 3) Aplica as renomeações na coluna tipo_delito para uniformização.
ocorrencias["tipo_delito"] = ocorrencias["tipo_delito"].replace(renomear_delitos)

# 4) Confirma as mudanças imprimindo os tipos únicos padronizados.
print(sorted(ocorrencias["tipo_delito"].unique()))

# --- CRIAÇÃO DA VERSÃO "LONG" DAS OCORRÊNCIAS (MELT) ---
# OBJETIVO: Transformar colunas mensais (jan..dez) em linhas (mes, quantidade) para análises temporais.


colunas_essenciais = ['idOcorrenciaMensal', 'idDelito', 'idDistrito', 'ano',
                      'tipo_delito', 'id_tipo_delito', 'id_grupo_delito'] + \
                     [mes for mes in ['janeiro','fevereiro','marco','abril','maio',
                                      'junho','julho','agosto','setembro','outubro',
                                      'novembro','dezembro']]

# 1) Seleciona as colunas essenciais (asegura que existam no dataset).
ocorrencias_filtradas = ocorrencias[colunas_essenciais].copy()

# 2) Melt para transformar meses em uma coluna 'mes' e valores em 'quantidade'.
ocorrencias_long = ocorrencias_filtradas.melt(
    id_vars=['idOcorrenciaMensal','idDelito','idDistrito','ano','tipo_delito','id_tipo_delito','id_grupo_delito'],
    value_vars=['janeiro','fevereiro','marco','abril','maio','junho','julho','agosto','setembro','outubro','novembro','dezembro'],
    var_name='mes',
    value_name='quantidade'
)

# 3) Criar mes_abrev no formato 'jan/19' para visualizações e agregações rápidas.
meses_abreviados = {
    'janeiro': 'jan', 'fevereiro': 'fev', 'marco': 'mar', 'abril': 'abr',
    'maio': 'mai', 'junho': 'jun', 'julho': 'jul', 'agosto': 'ago',
    'setembro': 'set', 'outubro': 'out', 'novembro': 'nov', 'dezembro': 'dez'
}
ocorrencias_long['mes_abrev'] = ocorrencias_long['mes'].map(meses_abreviados) + '/' + ocorrencias_long['ano'].astype(str).str[-2:]

# --- FUNÇÃO SEGURO PARA PADRONIZAR COLUNAS DE DATA EM ABAS DO QUINTOANDAR ---
# OBJETIVO: Detectar colunas de data com nomes diferentes e padronizá-las em 'ano', 'mes' e 'mes_abrev'.
def padroniza_mes_ano_seguro(df):
    df = df.copy()

    # 1) Lista de possíveis nomes que a coluna de data pode ter.
    possiveis_nomes = ['Mês/Ano', 'Mês / Ano', 'mes_ano']
    coluna_data = None
    for nome in possiveis_nomes:
        if nome in df.columns:
            coluna_data = nome
            break

    # 2) Se nenhuma coluna de data for encontrada, retorna o df sem alterações (robustez).
    if coluna_data is None:
        print("Nenhuma coluna de data encontrada, pulando essa aba.")
        return df

    # 3) Converte a coluna encontrada para datetime com errors='coerce' (valores inválidos viram NaT).
    df[coluna_data] = pd.to_datetime(df[coluna_data], errors='coerce')

    # 4) Cria colunas separadas de ano, mes (número) e mes_abrev (ex: 'jan/19').
    df['ano'] = df[coluna_data].dt.year
    df['mes'] = df[coluna_data].dt.month
    df['mes_abrev'] = df[coluna_data].dt.strftime('%b/%y').str.lower()

    return df

# OBJETIVO: Aplicar padronização temporal a todas as abas do quintoAndar para evitar erros posteriores.
for aba, df in quintoAndar.items():
    quintoAndar[aba] = padroniza_mes_ano_seguro(df)

# OBJETIVO: Visualizar algumas linhas da aba "Dados Gerais" já padronizada (checagem rápida).
quintoAndar['Dados Gerais'].head()

# --- TRANSFORMAÇÃO LONG DAS ABAS DO QUINTOANDAR ---
# OBJETIVO: Para cada aba do quintoAndar, transformar colunas numéricas em formato longo (variável/valor)
# para facilitar análises de séries temporais e comparação entre abas.

meses_pt = {
    1: "janeiro", 2: "fevereiro", 3: "março", 4: "abril", 5: "maio", 6: "junho",
    7: "julho", 8: "agosto", 9: "setembro", 10: "outubro", 11: "novembro", 12: "dezembro"
}

quintoAndar_long = {}

for aba, df in quintoAndar.items():
    df = df.copy()  # não altera o original

    # 1) Se houver coluna 'Mês/Ano', converte e separa em 'mes' e 'ano'.
    if 'Mês/Ano' in df.columns:
        df['Mês/Ano'] = pd.to_datetime(df['Mês/Ano'], errors='coerce')
        df['mes'] = df['Mês/Ano'].dt.month.map(meses_pt)
        df['ano'] = df['Mês/Ano'].dt.year
        df = df.drop(columns=['Mês/Ano'])

    # 2) Identifica colunas que são "fixas" (não-numéricas) e colunas de valores a serem derretidas.
    colunas_fixas = [c for c in df.columns if df[c].dtype == 'O' or c in ['mes','ano']]
    colunas_valores = [c for c in df.columns if c not in colunas_fixas]

    # 3) Se existirem colunas de valores, aplica melt; caso contrário, mantém o df como está.
    if colunas_valores:
        df_long = df.melt(id_vars=colunas_fixas, value_vars=colunas_valores,
                          var_name='variavel', value_name='valor')
    else:
        df_long = df

    quintoAndar_long[aba] = df_long

# OBJETIVO: Exibir exemplo da primeira aba transformada para validação.
quintoAndar_long[list(quintoAndar_long.keys())[0]].head()

# 1) existe a variável ocorrencias_long?
if 'ocorrencias_long' in globals() and isinstance(ocorrencias_long, pd.DataFrame):
    print("ocorrencias_long encontrada ")
    print("Shape:", ocorrencias_long.shape)
    display(ocorrencias_long.head())
    print("\nColunas:", ocorrencias_long.columns.tolist())
    print("\nMeses únicos:", sorted(ocorrencias_long['mes'].unique()))
    print("\nEx.: soma por mês (ver se faz sentido):")
    display(ocorrencias_long.groupby('mes')['quantidade'].sum().sort_index())
else:
    print("ocorrencias_long NÃO encontrada. Vou criar a versão long agora...")

    # 2) Cria a versão long (execute se a variável não existir)
    meses = ['janeiro','fevereiro','marco','abril','maio','junho',
             'julho','agosto','setembro','outubro','novembro','dezembro']

    # seleciona colunas essenciais (ajuste caso seus nomes sejam diferentes)
    colunas_essenciais = ['idOcorrenciaMensal','idDelito','idDistrito','ano',
                          'tipo_delito','id_tipo_delito','id_grupo_delito'] + meses

    # se alguma coluna não existir, pega apenas as que existem
    colunas_essenciais = [c for c in colunas_essenciais if c in ocorrencias.columns]

    ocorrencias_filtradas = ocorrencias[colunas_essenciais].copy()

    ocorrencias_long = ocorrencias_filtradas.melt(
        id_vars=[c for c in ['idOcorrenciaMensal','idDelito','idDistrito','ano','tipo_delito','id_tipo_delito','id_grupo_delito'] if c in ocorrencias_filtradas.columns],
        value_vars=[m for m in meses if m in ocorrencias_filtradas.columns],
        var_name='mes',
        value_name='quantidade'
    )

    # cria mes_abrev (jan/19) se existir 'ano'
    if 'ano' in ocorrencias_long.columns:
        ocorrencias_long['mes_abrev'] = ocorrencias_long['mes'].map({
            'janeiro':'jan','fevereiro':'fev','marco':'mar','abril':'abr','maio':'mai','junho':'jun',
            'julho':'jul','agosto':'ago','setembro':'set','outubro':'out','novembro':'nov','dezembro':'dez'
        }) + '/' + ocorrencias_long['ano'].astype(str).str[-2:]

    print("Versão long criada: ocorrencias_long")
    print("Shape:", ocorrencias_long.shape)
    display(ocorrencias_long.head())
    print("\nColunas:", ocorrencias_long.columns.tolist())

# --- RESUMO DAS ABAS TRANSFORMADAS (verificação) ---
# OBJETIVO: Mostrar chaves (nomes das abas) que foram convertidas para longo e um head de cada.
print("Chaves disponíveis no dicionário quintoAndar_long:\n")
print(list(quintoAndar_long.keys()))

# Mostrar um resumo de cada aba
for aba, df in quintoAndar_long.items():
    print(f"\n=== Aba: {aba} ===")
    print(f"Shape: {df.shape}")
    print("Colunas:", list(df.columns))
    display(df.head(5))

# --- CONSTRUÇÃO DE RANKINGS LONG (Ex.: Bairros valorizados) ---
# OBJETIVO: Extrair rankings (1º-5º) de abas específicas e padronizar o nome do bairro para comparação.
import unidecode

# Inicializa uma lista para guardar os dataframes transformados
rankings_long = []

# 1) Mapear quais abas representam quais tipos de ranking.
abas_rankings = {
    'Bairros valorizados': 'valorizado',
    'Bairros desvalorizados': 'desvalorizado',
    'Bairros mais caros': 'mais_caro'
}

# 2) Iterar nas abas conhecidas, derreter posições 1-5 em linhas e adicionar tipo_ranking.
for aba, tipo in abas_rankings.items():
    if aba in quintoAndar_long:
        df = quintoAndar_long[aba].copy()

        # a) Derrete colunas de posição em uma coluna 'bairro'.
        df_long = df.melt(
            id_vars=['ano', 'mes', 'mes_abrev'],
            value_vars=['1º lugar', '2º lugar', '3º lugar', '4º lugar', '5º lugar'],
            var_name='posicao',
            value_name='bairro'
        )

        # b) Marca o tipo de ranking (valorizado/desvalorizado/mais_caro).
        df_long['tipo_ranking'] = tipo

        # c) Limpa e padroniza os nomes dos bairros (strip, lower, remove acentos).
        df_long['bairro'] = (
            df_long['bairro']
            .astype(str)
            .str.strip()
            .str.lower()
            .apply(unidecode.unidecode)
        )

        rankings_long.append(df_long)

# 3) Concatena todos os rankings em um DataFrame único.
quintoAndar_rankings = pd.concat(rankings_long, ignore_index=True)

# 4) Exibe resumo do novo DataFrame de rankings.
print("quintoAndar_rankings criado com sucesso!")
print(f"Shape: {quintoAndar_rankings.shape}")
print(f"Colunas: {list(quintoAndar_rankings.columns)}\n")
print(quintoAndar_rankings.head(10))

# --- SALVANDO VERSÕES FINAIS (EXCEL/CSV) ---
# OBJETIVO: Criar pasta de saída e salvar principais datasets (versão Excel e CSV).

base_path = '/content/drive/MyDrive/Colab Notebooks/versao_final_dados'
os.makedirs(base_path, exist_ok=True)

arquivos = {
    "dados_limpos.xlsx": dados,
    "ocorrencias_original.xlsx": ocorrencias,
    "ocorrencias_long.xlsx": ocorrencias_long,
    "bairros_referencia.xlsx": bairros_ref,
    "quintoAndar_rankings.xlsx": quintoAndar_rankings
}

# 1) Salva cada DataFrame em um arquivo Excel separado.
for nome, df in arquivos.items():
    caminho = os.path.join(base_path, nome)
    df.to_excel(caminho, index=False)
    print(f"{nome} salvo com sucesso em: {caminho}")

# 2) Salva todas as abas de quintoAndar_long em um único arquivo Excel (cada aba uma sheet).
with pd.ExcelWriter(os.path.join(base_path, "quintoAndar_long_abas.xlsx")) as writer:
    for aba, df in quintoAndar_long.items():
        df.to_excel(writer, sheet_name=aba[:30], index=False)
    print("Todas as abas de quintoAndar_long salvas em um único arquivo Excel.")

# 3) Salva relatórios trimestrais em arquivo Excel multi-sheet.
with pd.ExcelWriter(os.path.join(base_path, "relatorios_trimestrais.xlsx")) as writer:
    for aba, df in relatorioTri.items():
        df.to_excel(writer, sheet_name=aba[:30], index=False)
    print("Todas as abas de relatorioTri salvas em um único arquivo Excel.")

# --- VERSÕES CSV ---
# OBJETIVO: Salvar versões CSV dos principais datasets (útil para interoperabilidade).
base_path = '/content/drive/MyDrive/Colab Notebooks/versao_final_dados'
os.makedirs(base_path, exist_ok=True)

arquivos_csv = {
    "dados_limpos.csv": dados,
    "ocorrencias_original.csv": ocorrencias,
    "ocorrencias_long.csv": ocorrencias_long,
    "bairros_referencia.csv": bairros_ref,
    "quintoAndar_rankings.csv": quintoAndar_rankings
}

for nome, df in arquivos_csv.items():
    caminho = os.path.join(base_path, nome)
    df.to_csv(caminho, index=False)
    print(f"{nome} salvo com sucesso em CSV: {caminho}")

# Salva cada aba de quintoAndar_long como CSV separado (um arquivo por aba).
for aba, df in quintoAndar_long.items():
    nome_arquivo = f"quintoAndar_long_{aba}.csv"
    caminho = os.path.join(base_path, nome_arquivo)
    df.to_csv(caminho, index=False)
    print(f"Aba '{aba}' salva em CSV: {caminho}")

# Salva relatórios trimestrais também em CSV separados.
for aba, df in relatorioTri.items():
    nome_arquivo = f"relatorioTri_{aba}.csv"
    caminho = os.path.join(base_path, nome_arquivo)
    df.to_csv(caminho, index=False)
    print(f"Aba '{aba}' de relatorioTri salva em CSV: {caminho}")

# --- RECARREGAR DADOS LIMPOS E CALCULAR PRECO POR M2 ---
# OBJETIVO: Recarregar arquivo salvo para garantir que a análise use a versão persistida.
dados_limpos = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/dados_limpos.xlsx')

# 1) Calcula preco por m² para cada imóvel (aluguel / área m²).
dados_limpos['preco_m2'] = dados_limpos['aluguel'] / dados_limpos['área m²']

# 2) Ranking de distritos pelo preço médio por m² (média agregada por id_distrito).
ranking_precos = (
    dados_limpos.groupby('id_distrito')['preco_m2']
    .mean()
    .sort_values(ascending=False)
    .reset_index()
)

ranking_precos.rename(columns={'preco_m2': 'preco_m2_medio'}, inplace=True)

# 3) Mostrar os 15 distritos mais caros por m² (checagem).
print("Top 15 distritos mais caros por m²:")
display(ranking_precos.head(15))

# --- PREPARAR RANKING DE CRIMINALIDADE (EXEMPLO: MAIO/2023) ---
# OBJETIVO: Criar ranking de criminalidade para um mês/ano específico para cruzamento com preços.
dados_para_cruzamento = ranking_precos.copy()

# Filtra apenas os dados de maio de 2023
ocorrencias_maio_2023 = ocorrencias_long[
    (ocorrencias_long['ano'] == 2023) &
    (ocorrencias_long['mes'] == 'maio')
]

# Agrupa por distrito e soma todas as ocorrências
ranking_criminalidade = (
    ocorrencias_maio_2023
    .groupby('idDistrito')['quantidade']
    .sum()
    .reset_index()
    .rename(columns={'quantidade': 'total_ocorrencias'})
    .sort_values(by='total_ocorrencias', ascending=False)
)

# Exibe os top 15 distritos mais problemáticos
top_15_criminalidade = ranking_criminalidade.head(15)
print("Top 15 distritos por criminalidade em maio/2023:")
display(top_15_criminalidade)

# --- ENRIQUECER bairros_ref COM NOMES DE DISTRITO ---
# OBJETIVO: Mapear id_distrito para nome_distrito usando a aba 'Planilha1' da tabela distritos_bairros.
# Seleciona colunas relevantes de distritos_bairros
distritos_nomes = distritos_bairros["Planilha1"][["ID_Distrito", "Nome"]].copy()
distritos_nomes.rename(columns={"ID_Distrito": "id_distrito", "Nome": "nome_distrito"}, inplace=True)

# 1 Padroniza os nomes (sem acento e minúsculo)
distritos_nomes["nome_distrito"] = distritos_nomes["nome_distrito"].apply(lambda x: unidecode.unidecode(str(x)).lower())

# 2 Merge com bairros_ref
bairros_ref = bairros_ref.merge(distritos_nomes, on="id_distrito", how="left")

# 3 Visualiza o resultado
bairros_ref.head()

# --- INSPEÇÃO DAS COLUNAS DA ABA 'Planilha1' ---
# OBJETIVO: Mostrar nomes das colunas presentes na aba para ajudar no debug.
distritos_bairros['Planilha1'].columns

# --- UNIR RANKING DE PREÇOS E CRIMINALIDADE E ENRIQUECER COM NOMES ---
# OBJETIVO: Fazer merges para ter uma tabela consolidada (preço médio m2 vs total de ocorrências) com nome do distrito.
# Carregue as bibliotecas de visualização
import matplotlib.pyplot as plt
import seaborn as sns

# Renomeia a coluna no ranking de criminalidade para padronizar
ranking_criminalidade.rename(columns={'idDistrito': 'id_distrito'}, inplace=True)

# Une os dois rankings usando o id_distrito como chave
# 'inner' garante que só teremos distritos presentes em AMBAS as listas
analise_final = pd.merge(ranking_precos, ranking_criminalidade, on='id_distrito', how='inner')

# Vamos enriquecer essa tabela com os nomes dos distritos que você já preparou
# Supondo que 'bairros_ref' tenha as colunas 'id_distrito' e 'nome_distrito'
nomes_distritos = bairros_ref[['id_distrito', 'nome_distrito']].drop_duplicates()
analise_final = pd.merge(analise_final, nomes_distritos, on='id_distrito', how='left')


print("Tabela final pronta para análise:")
display(analise_final.head())

# --- DEBUG DE COLUNAS E NORMALIZAÇÃO ---
# Este comando vai listar os nomes exatos de todas as colunas
print(analise_final.columns)

# Checar colunas existentes (útil para debug)
print("Colunas ranking_precos:", ranking_precos.columns.tolist())
print("Colunas ranking_criminalidade:", ranking_criminalidade.columns.tolist())
print("Colunas bairros_ref:", bairros_ref.columns.tolist())

# --- Garantir nomes consistentes ---
# Se ranking_precos tiver 'preco_m2_medio', renomeia para 'preco_m2' para consistência
if 'preco_m2_medio' in ranking_precos.columns and 'preco_m2' not in ranking_precos.columns:
    ranking_precos = ranking_precos.rename(columns={'preco_m2_medio': 'preco_m2'})

# Renomear id na criminalidade se necessário
if 'idDistrito' in ranking_criminalidade.columns:
    ranking_criminalidade = ranking_criminalidade.rename(columns={'idDistrito': 'id_distrito'})

# Forçar tipos (ajuda em merges)
ranking_precos['id_distrito'] = ranking_precos['id_distrito'].astype(int)
ranking_criminalidade['id_distrito'] = ranking_criminalidade['id_distrito'].astype(int)

# Preparar nomes dos distritos (garantir colunas)
if 'nome_distrito' not in bairros_ref.columns:
    # tenta encontrar qualquer coluna parecida
    possiveis = [c for c in bairros_ref.columns if 'nome' in c.lower() or 'distrito' in c.lower()]
    print("Possíveis colunas de nome em bairros_ref:", possiveis)
else:
    possiveis = ['nome_distrito']

# Criar nomes_distritos usando a coluna apropriada
nome_col = possiveis[0] if possiveis else None
if nome_col:
    nomes_distritos = bairros_ref[['id_distrito', nome_col]].drop_duplicates().rename(columns={nome_col: 'nome_distrito'})
else:
    # fallback: apenas id
    nomes_distritos = bairros_ref[['id_distrito']].drop_duplicates()
    nomes_distritos['nome_distrito'] = None

# --- Merge explícito com sufixos para evitar surpresas ---
analise_final = pd.merge(ranking_precos, ranking_criminalidade, on='id_distrito', how='inner', suffixes=('_preco','_crime'))

# Merge com nomes dos distritos (usar left para não perder dados já cruzados)
analise_final = pd.merge(analise_final, nomes_distritos, on='id_distrito', how='left')

# Mostrar colunas após merges
print("Colunas analise_final após merges:", analise_final.columns.tolist())

# Normalizar coluna de nome (se houver duplicatas por sufixos)
if 'nome_distrito_x' in analise_final.columns and 'nome_distrito_y' in analise_final.columns:
    analise_final['nome_distrito'] = analise_final['nome_distrito_x'].fillna(analise_final['nome_distrito_y'])
    analise_final = analise_final.drop(columns=['nome_distrito_x','nome_distrito_y'])
elif 'nome_distrito' not in analise_final.columns:
    # tenta recuperar qualquer coluna que contenha "nome"
    for c in analise_final.columns:
        if 'nome' in c.lower():
            analise_final = analise_final.rename(columns={c: 'nome_distrito'})
            break

# Verificar se temos a coluna de preço correta
if 'preco_m2' not in analise_final.columns:
    # tenta usar qualquer coluna que contenha 'preco' e 'm2'
    for c in analise_final.columns:
        if 'preco' in c.lower() and ('m2' in c.lower() or 'm²' in c.lower()):
            analise_final = analise_final.rename(columns={c: 'preco_m2'})
            break

print("Colunas finais (após normalização):", analise_final.columns.tolist())

# Drop de linhas com NaN nas colunas cruciais antes do gráfico
analise_final = analise_final.dropna(subset=['preco_m2', 'total_ocorrencias'])

# --- GRÁFICO PREÇO POR M2 vs CRIMINALIDADE (MAIO/2023) ---
# OBJETIVO: Visualizar possível relação entre preço por m² e criminalidade por distrito (dispersão).


plt.figure(figsize=(14, 8))
sns.scatterplot(data=analise_final, x='total_ocorrencias', y='preco_m2')
plt.title('Relação entre Preço por m² e Criminalidade por Distrito (Maio/2023)', fontsize=16)
plt.xlabel('Número Total de Ocorrências Criminais', fontsize=12)
plt.ylabel('Preço Médio por Metro Quadrado (R$)', fontsize=12)
plt.grid(True)
plt.show()

# OBJETIVO: Calcular correlação entre total de ocorrências e preço por m² (medida rápida de associação).
correlacao = analise_final['total_ocorrencias'].corr(analise_final['preco_m2'])
print(f"\nA correlação entre o total de ocorrências e o preço médio por m² é: {correlacao:.2f}")

# --- (REPETIÇÃO/VERIFICAÇÃO) EXIBIÇÃO DAS ABAS (opcional, para debug) ---
# OBJETIVO: Re-exibir chaves e amostras para inspeção manual.
print("Chaves disponíveis no dicionário quintoAndar_long:\n")
print(list(quintoAndar_long.keys()))

# Mostrar um resumo de cada aba
for aba, df in quintoAndar_long.items():
    print(f"\n=== Aba: {aba} ===")
    print(f"Shape: {df.shape}")
    print("Colunas:", list(df.columns))
    display(df.head(5))

# --- PREPARAR MAPA (BAIRRO -> ID_DISTRITO) PARA RANKINGS ---
# OBJETIVO: Criar mapeamento com colunas corretas para facilitar merges posteriores.
# Colunas necessárias: 'Bairro' (limpo, em minúsculo e sem acento) e 'id_distrito'
bairros_ref_mapa = bairros_ref[['Bairro', 'id_distrito']].copy().drop_duplicates()
bairros_ref_mapa.rename(columns={'Bairro': 'bairro'}, inplace=True) # Renomeia para facilitar o merge

# Faz a junção para adicionar o id_distrito ao seu dataframe de rankings
rankings_com_distrito = pd.merge(
    quintoAndar_rankings,
    bairros_ref_mapa,
    on='bairro',
    how='left' # Usamos 'left' para manter todos os bairros do ranking, mesmo que algum não seja encontrado
)

# Vamos verificar se algum bairro do ranking ficou sem um id_distrito
print(f"Bairros do ranking sem ID de distrito correspondente: {rankings_com_distrito['id_distrito'].isnull().sum()}")

# Remove os que não foram encontrados e converte o ID para inteiro
rankings_com_distrito.dropna(subset=['id_distrito'], inplace=True)
rankings_com_distrito['id_distrito'] = rankings_com_distrito['id_distrito'].astype(int)

print(" ID do Distrito adicionado com sucesso aos rankings!")
display(rankings_com_distrito.head())

# Recria o merge, mas desta vez vamos capturar os que não combinaram
merge_completo = pd.merge(
    quintoAndar_rankings,
    bairros_ref_mapa,
    on='bairro',
    how='left',
    indicator=True # Adicionamos este indicador
)

# Filtra apenas as linhas que existem apenas na tabela da esquerda (QuintoAndar)
bairros_perdidos = merge_completo[merge_completo['_merge'] == 'left_only']

# Pega a lista de nomes únicos que não foram encontrados
lista_nomes_perdidos = sorted(bairros_perdidos['bairro'].unique())

print(f"Encontramos {len(lista_nomes_perdidos)} nomes de bairros únicos que não foram encontrados na sua referência.")
print("Aqui estão eles:")
print(lista_nomes_perdidos)

# Exemplo de dicionário de correção
mapa_correcoes = {
    'cid.sao francisco': 'cidade sao francisco',
    'jardim america': 'jardim europa',
    'jd. analisa franco': 'vila formosa',
    'jd. ester yolanda': 'jardim ester yolanda',
    'jd. europa': 'jardim europa',
    'pq. novo mundo': 'parque novo mundo'
}

# Aplica as correções na coluna 'bairro' do seu DataFrame original de rankings
print("Aplicando correções...")
quintoAndar_rankings['bairro'] = quintoAndar_rankings['bairro'].replace(mapa_correcoes)
print(" Correções aplicadas!")

bairros_ref_mapa = bairros_ref[['Bairro', 'id_distrito']].copy().drop_duplicates()
bairros_ref_mapa.rename(columns={'Bairro': 'bairro'}, inplace=True)

rankings_com_distrito = pd.merge(
    quintoAndar_rankings,
    bairros_ref_mapa,
    on='bairro',
    how='left'
)

print(f"Bairros do ranking sem ID de distrito correspondente: {rankings_com_distrito['id_distrito'].isnull().sum()}")

rankings_com_distrito.dropna(subset=['id_distrito'], inplace=True)
rankings_com_distrito['id_distrito'] = rankings_com_distrito['id_distrito'].astype(int)

print("ID do Distrito adicionado com sucesso aos rankings!")
display(rankings_com_distrito.head())

merge_completo = pd.merge(
    quintoAndar_rankings,
    bairros_ref_mapa,
    on='bairro',
    how='left',
    indicator=True
)

bairros_perdidos = merge_completo[merge_completo['_merge'] == 'left_only']

lista_nomes_perdidos = sorted(bairros_perdidos['bairro'].unique())

print(f"Encontramos {len(lista_nomes_perdidos)} nomes de bairros únicos que não foram encontrados na sua referência.")
print("Aqui estão eles:")
print(lista_nomes_perdidos)

mapa_correcoes = {
    'cid. sao francisco': 'cidade sao francisco',
    'jd. analia franco': 'vila formosa',
}

print("Aplicando correções...")
quintoAndar_rankings['bairro'] = quintoAndar_rankings['bairro'].replace(mapa_correcoes)
print(" Correções aplicadas!")

bairros_ref_mapa = bairros_ref[['Bairro', 'id_distrito']].copy().drop_duplicates()
bairros_ref_mapa.rename(columns={'Bairro': 'bairro'}, inplace=True)

rankings_com_distrito = pd.merge(
    quintoAndar_rankings,
    bairros_ref_mapa,
    on='bairro',
    how='left'
)

print(f"Bairros do ranking sem ID de distrito correspondente: {rankings_com_distrito['id_distrito'].isnull().sum()}")

rankings_com_distrito.dropna(subset=['id_distrito'], inplace=True)
rankings_com_distrito['id_distrito'] = rankings_com_distrito['id_distrito'].astype(int)

print(" ID do Distrito adicionado com sucesso aos rankings!")
display(rankings_com_distrito.head())

merge_completo = pd.merge(
    quintoAndar_rankings,
    bairros_ref_mapa,
    on='bairro',
    how='left',
    indicator=True
)

bairros_perdidos = merge_completo[merge_completo['_merge'] == 'left_only']

lista_nomes_perdidos = sorted(bairros_perdidos['bairro'].unique())

print(f"Encontramos {len(lista_nomes_perdidos)} nomes de bairros únicos que não foram encontrados na sua referência.")
print("Aqui estão eles:")
print(lista_nomes_perdidos)

# --- PASSO 2: A JUNÇÃO TEMPORAL COM OS CRIMES ---
# OBJETIVO: Agregar ocorrências por idDistrito/ano/mes_num para permitir join temporal com rankings (ano+mes).
# 1. Prepara a tabela de crimes (como planejamos antes)
mapa_meses = { 'janeiro': 1, 'fevereiro': 2, 'marco': 3, 'abril': 4, 'maio': 5, 'junho': 6, 'julho': 7, 'agosto': 8, 'setembro': 9, 'outubro': 10, 'nov': 11, 'dezembro': 12 }
ocorrencias_long['mes_num'] = ocorrencias_long['mes'].map(mapa_meses)
crimes_temporal = ocorrencias_long.groupby(['idDistrito', 'ano', 'mes_num'])['quantidade'].sum().reset_index()
crimes_temporal.rename(columns={'idDistrito': 'id_distrito', 'quantidade': 'total_ocorrencias'}, inplace=True)

# 2. Prepara a tabela de rankings para o merge (já tem ano, agora precisa do mes_num)
# Supondo que a coluna 'mes' em 'quintoAndar_rankings' seja o NOME do mês
rankings_com_distrito['mes_num'] = rankings_com_distrito['mes'].map(mapa_meses)
rankings_com_distrito.dropna(subset=['id_distrito'], inplace=True) # Garante que não tem bairros sem distrito
rankings_com_distrito['id_distrito'] = rankings_com_distrito['id_distrito'].astype(int)


# 3. A GRANDE JUNÇÃO
analise_rankings_temporal = pd.merge(
    rankings_com_distrito,
    crimes_temporal,
    on=['id_distrito', 'ano', 'mes_num'],
    how='left'
)

print(" Junção temporal entre rankings e crimes realizada!")
display(analise_rankings_temporal.head())

# --- PASSO FINAL: LIMPEZA E ANÁLISE DOS RANKINGS ---
# OBJETIVO: Remover linhas sem dados de crime e calcular métricas por tipo de ranking.
# 1. Vamos verificar a extensão do problema. Quantas linhas ficaram sem dados de crime?
print(f"Total de linhas na tabela antes da limpeza: {len(analise_rankings_temporal)}")
linhas_com_nan = analise_rankings_temporal['total_ocorrencias'].isnull().sum()
print(f"Linhas com dados de crime ausentes (NaN): {linhas_com_nan}")

# 2. Remove as linhas onde o 'total_ocorrencias' é NaN.
analise_final_limpa = analise_rankings_temporal.dropna(subset=['total_ocorrencias'])
print(f"Total de linhas após a limpeza: {len(analise_final_limpa)}")

# 3. Agora, com a tabela limpa, vamos para a análise!
# Agrupa por tipo de ranking e calcula a média de ocorrências.
media_crimes_por_ranking = analise_final_limpa.groupby('tipo_ranking')['total_ocorrencias'].mean().sort_values(ascending=False)

print("\n\n--- RESULTADO FINAL ---")
print("Média de ocorrências criminais mensais por tipo de ranking:")
print(media_crimes_por_ranking)

# Plotar média por categoria de ranking.
plt.figure(figsize=(10, 6))
sns.barplot(x=media_crimes_por_ranking.index, y=media_crimes_por_ranking.values, palette='coolwarm')
plt.title('Média de Ocorrências Criminais por Categoria de Ranking de Bairro', fontsize=15)
plt.ylabel('Média Mensal de Ocorrências no Distrito', fontsize=12)
plt.xlabel('Tipo de Ranking', fontsize=12)
plt.show()

# --- ANÁLISE MACRO TEMPORAL: PREÇO MÉDIO SP vs CRIMES TOTAIS ---
# OBJETIVO: Construir séries macro (SP inteiro) de preço médio por m² e total de crimes para análise temporal.

print("--- Passo 1: Preparando os Dados de Preço (Macro) ---")
precos_macro_df = quintoAndar_long['Dados Gerais'].copy()
precos_macro_df.rename(columns={'Preço Médio do m² (R$)': 'preco_medio_sp'}, inplace=True)

# --- CORREÇÃO FINAL APLICADA AQUI ---
# Força a conversão da coluna para número. Textos como 'N/I' virarão NaN.
precos_macro_df['preco_medio_sp'] = pd.to_numeric(precos_macro_df['preco_medio_sp'], errors='coerce')
# Agora removemos qualquer linha que tenha ficado com preço NaN (seja um NaN original ou um 'N/I' convertido)
precos_macro_df.dropna(subset=['preco_medio_sp'], inplace=True)
# 2) Mapear meses e criar coluna ano_mes no formato 'YYYY-MM'
mapa_meses_num = {
    'janeiro': 1, 'fevereiro': 2, 'marco': 3, 'março': 3, 'abril': 4, 'maio': 5, 'junho': 6,
    'julho': 7, 'agosto': 8, 'setembro': 9, 'outubro': 10, 'novembro': 11, 'dezembro': 12
}
precos_macro_df['mes'] = precos_macro_df['mes'].astype(str).str.lower().str.strip()
precos_macro_df['mes_num'] = precos_macro_df['mes'].map(mapa_meses_num)
precos_macro_df.dropna(subset=['mes_num'], inplace=True)
precos_macro_df['mes_num'] = precos_macro_df['mes_num'].astype(int)
precos_macro_df['ano_mes'] = precos_macro_df['ano'].astype(str) + '-' + precos_macro_df['mes_num'].astype(str).str.zfill(2)
precos_macro_df = precos_macro_df[['ano_mes', 'preco_medio_sp']]

print(" Tabela de Preços Macro pronta!")


print("\n--- Passo 2: Preparando os Dados de Crime (Macro) ---")
# OBJETIVO: Preparar série temporal agregada de crimes em SP por ano_mes.
ocorrencias_long['mes_num'] = ocorrencias_long['mes'].map(mapa_meses_num)
ocorrencias_long.dropna(subset=['mes_num'], inplace=True)
ocorrencias_long['mes_num'] = ocorrencias_long['mes_num'].astype(int)
ocorrencias_long['ano_mes'] = ocorrencias_long['ano'].astype(str) + '-' + ocorrencias_long['mes_num'].astype(str).str.zfill(2)
crimes_macro_df = ocorrencias_long.groupby('ano_mes')['quantidade'].sum().reset_index()
crimes_macro_df.rename(columns={'quantidade': 'total_ocorrencias_sp'}, inplace=True)

print(" Tabela de Crimes Macro pronta!")


print("\n--- Passo 3: Junção Final e Visualização Panorâmica ---")
# OBJETIVO: Juntar séries macro e preparar para plot.
analise_macro_final = pd.merge(precos_macro_df, crimes_macro_df, on='ano_mes', how='inner')
analise_macro_final = analise_macro_final.sort_values('ano_mes')

print(" Tabela Macro Final pronta para visualização!")


# --- O GRÁFICO FINAL ---
# OBJETIVO: Plotar série temporal comparando preço médio por m² e total de ocorrências (duas y-axes).
fig, ax1 = plt.subplots(figsize=(15, 8))

color = 'tab:blue'
ax1.set_xlabel('Data (Ano-Mês)', fontsize=12)
ax1.set_ylabel('Preço Médio por m² (R$)', color=color, fontsize=14)
ax1.plot(analise_macro_final['ano_mes'], analise_macro_final['preco_medio_sp'], color=color, marker='o', label='Preço Médio m²')
ax1.tick_params(axis='y', labelcolor=color)
ax1.tick_params(axis='x', rotation=45)

ax2 = ax1.twinx()
color = 'tab:red'
ax2.set_ylabel('Total de Ocorrências Criminais', color=color, fontsize=14)
ax2.plot(analise_macro_final['ano_mes'], analise_macro_final['total_ocorrencias_sp'], color=color, marker='x', linestyle='--', label='Total de Ocorrências')
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Evolução do Preço Médio de Imóveis vs. Criminalidade Total em São Paulo', fontsize=18)
fig.tight_layout()
plt.grid(True, linestyle='--', alpha=0.6)
fig.legend(loc="upper right", bbox_to_anchor=(0.9, 0.9))
plt.show()

# --- SEPARAÇÃO DE TIPOS DE CRIMES EM "CESTAS" ---
# OBJETIVO: Categorizar crimes em cestas (violentos / contra o patrimônio) para análises específicas.
lista_de_crimes = sorted(ocorrencias['tipo_delito'].unique())

print("Tipos de crime disponíveis no seu dataset:")
for crime in lista_de_crimes:
    print(f"- {crime}")

print("--- Iniciando a separação dos crimes por tipo ---")

# CESTA 1: Crimes Violentos (Alto Impacto na Percepção de Segurança)
crimes_violentos_lista = [
    'HOMICÍDIO DOLOSO', 'LATROCÍNIO', 'ESTUPRO', 'ESTUPRO DE VULNERÁVEL',
    'LESÃO CORPORAL SEGUIDA DE MORTE', 'TENTATIVA DE HOMICÍDIO', 'LESÃO CORPORAL DOLOSA',
    'HOMICÍDIO DOLOSO POR ACIDENTE DE TRÂNSITO'
]

# CESTA 2: Crimes Contra o Patrimônio (Ligados à Circulação e Oportunidade)
crimes_patrimonio_lista = [
    'ROUBO', 'FURTO', 'ROUBO DE VEÍCULO', 'FURTO DE VEÍCULO',
    'ROUBO A BANCO', 'ROUBO DE CARGA'
]

# Filtra as ocorrências para cada cesta
ocorrencias_violentos = ocorrencias_long[ocorrencias_long['tipo_delito'].isin(crimes_violentos_lista)]
ocorrencias_patrimonio = ocorrencias_long[ocorrencias_long['tipo_delito'].isin(crimes_patrimonio_lista)]

# Cria a tabela temporal APENAS com crimes violentos
crimes_violentos_temporal = (
    ocorrencias_violentos.groupby(['idDistrito', 'ano', 'mes_num'])['quantidade']
    .sum().reset_index()
    .rename(columns={'idDistrito': 'id_distrito', 'quantidade': 'total_ocorrencias_violentos'})
)

# Cria a tabela temporal APENAS com crimes contra o patrimônio
crimes_patrimonio_temporal = (
    ocorrencias_patrimonio.groupby(['idDistrito', 'ano', 'mes_num'])['quantidade']
    .sum().reset_index()
    .rename(columns={'idDistrito': 'id_distrito', 'quantidade': 'total_ocorrencias_patrimonio'})
)

print("\n Novas tabelas de crimes por tipo criadas com sucesso!")
print("\n--- Prévia: Crimes Violentos (Temporal) ---")
display(crimes_violentos_temporal.head())

print("\n--- Prévia: Crimes Contra o Patrimônio (Temporal) ---")
display(crimes_patrimonio_temporal.head())

# --- ANÁLISE 1: RANKINGS vs. CRIMES VIOLENTOS ---
# OBJETIVO: Juntar rankings_com_distrito com crimes violentos temporais, calcular médias por categoria e plotar.
print("\n\n--- ANÁLISE 1: RANKINGS vs. CRIMES VIOLENTOS ---")

# 1. Faz a junção temporal com os crimes violentos
analise_violentos = pd.merge(
    rankings_com_distrito,
    crimes_violentos_temporal,
    on=['id_distrito', 'ano', 'mes_num'],
    how='left'
)

# 2. Limpa as linhas onde não houve correspondência de crime
analise_violentos.dropna(subset=['total_ocorrencias_violentos'], inplace=True)
print(f"Total de {len(analise_violentos)} registros analisados para crimes violentos.")

# 3. Calcula a média de ocorrências por tipo de ranking
media_violentos_por_ranking = analise_violentos.groupby('tipo_ranking')['total_ocorrencias_violentos'].mean().sort_values(ascending=False)

print("\nMédia de ocorrências de CRIMES VIOLENTOS por tipo de ranking:")
print(media_violentos_por_ranking)

# 4. Visualiza o resultado para Crimes Violentos
plt.figure(figsize=(10, 6))
sns.barplot(x=media_violentos_por_ranking.index, y=media_violentos_por_ranking.values,
            palette='viridis', hue=media_violentos_por_ranking.index, legend=False)
plt.title('Média de Crimes VIOLENTOS por Categoria de Ranking de Bairro', fontsize=16)
plt.ylabel('Média Mensal de Ocorrências Violentas no Distrito', fontsize=12)
plt.xlabel('Tipo de Ranking', fontsize=12)
plt.show()

# --- ANÁLISE 2: RANKINGS vs. CRIMES CONTRA O PATRIMÔNIO ---
# OBJETIVO: Mesmo processo para crimes contra o patrimônio.
print("\n\n--- ANÁLISE 2: RANKINGS vs. CRIMES CONTRA O PATRIMÔNIO ---")

# 1. Faz a junção temporal com os crimes contra o patrimônio
analise_patrimonio = pd.merge(
    rankings_com_distrito,
    crimes_patrimonio_temporal,
    on=['id_distrito', 'ano', 'mes_num'],
    how='left'
)

# 2. Limpa as linhas onde não houve correspondência de crime
analise_patrimonio.dropna(subset=['total_ocorrencias_patrimonio'], inplace=True)
print(f"Total de {len(analise_patrimonio)} registros analisados para crimes contra o patrimônio.")

# 3. Calcula a média de ocorrências por tipo de ranking
media_patrimonio_por_ranking = analise_patrimonio.groupby('tipo_ranking')['total_ocorrencias_patrimonio'].mean().sort_values(ascending=False)

print("\nMédia de ocorrências de CRIMES CONTRA O PATRIMÔNIO por tipo de ranking:")
print(media_patrimonio_por_ranking)

# 4. Visualiza o resultado para Crimes Contra o Patrimônio
plt.figure(figsize=(10, 6))
sns.barplot(x=media_patrimonio_por_ranking.index, y=media_patrimonio_por_ranking.values,
            palette='plasma', hue=media_patrimonio_por_ranking.index, legend=False)
plt.title('Média de Crimes CONTRA O PATRIMÔNIO por Categoria de Ranking', fontsize=16)
plt.ylabel('Média Mensal de Ocorrências Contra o Patrimônio no Distrito', fontsize=12)
plt.xlabel('Tipo de Ranking', fontsize=12)
plt.show()

# --- ESTATÍSTICAS DESCRITIVAS FINAIS ---
# OBJETIVO: Gerar tabelas de estatísticas descritivas para as principais variáveis analisadas.
print("\n\n--- ESTATÍSTICAS DESCRITIVAS COMPLETAS ---")

# Seleciona as colunas numéricas usando o nome correto 'preco_m2'
colunas_para_analise = ['preco_m2', 'total_ocorrencias']

# Gera a tabela de estatísticas descritivas para a análise de Maio/2023
print("\nEstatísticas para a Análise de Maio/2023:")
# 2) Verifica se colunas existem e, se sim, descreve.
colunas_existentes = [col for col in colunas_para_analise if col in analise_final.columns]
if colunas_existentes:
    display(analise_final[colunas_existentes].describe())
else:
    print("Nenhuma das colunas para análise foi encontrada no DataFrame 'analise_final'.")


# Gera a tabela de estatísticas para os dados de crimes por tipo
print("\nEstatísticas para a Análise por Tipo de Crime (médias por ranking):")

# Verifica se os dataframes de análise por tipo existem antes de descrevê-los
if 'analise_violentos' in locals():
    print("\nCrimes Violentos:")
    stats_violentos = pd.DataFrame(analise_violentos.groupby('tipo_ranking')['total_ocorrencias_violentos'].describe())
    display(stats_violentos)
else:
    print("\nDataFrame 'analise_violentos' não encontrado.")

if 'analise_patrimonio' in locals():
    print("\nCrimes Contra o Patrimônio:")
    stats_patrimonio = pd.DataFrame(analise_patrimonio.groupby('tipo_ranking')['total_ocorrencias_patrimonio'].describe())
    display(stats_patrimonio)
else:
    print("\nDataFrame 'analise_patrimonio' não encontrado.")